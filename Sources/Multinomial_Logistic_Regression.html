<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>Multinomial Logistic Regression</title>

    <!-- Linking css and javascript -->
    <link rel="stylesheet" id="cssfile" href="../styles/lightmode.css">
    <script type="text/javascript" src="../scripts/functionality.js"></script>

    <!-- Linking for MathJax to load -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

</head>

<body>
    <!-- <button id="dmbutton" class="fixedContainer">Dark Mode</button> -->

    <div class="container">
        <script language="javascript" type="text/javascript" src="toc.txt"></script>


        <div class="content">
            <script language="javascript" type="text/javascript" src="mini_toc.txt"></script>

            <div class="actual_content">

                <h1>Multinomial Logistic Regression</h1>
    
                <h2 id="Section1">Mathematical Representation</h2>
                <p>Suppose, a p dimensional vector \(x\) is our explanatory variable and \(Y\) is the dependent variable. Now
                unlike Binomial logistic regression where there were two independent classes in \(Y\), here the number of
                classes is more than 2. For better explanation without doing much calculation we are assuming that, there
                are 3 classes in the response variable \(Y\). Let us denote the classes by \(Y\) = 1, \(Y\) = 2 and \(Y\) = 3.</p>
    
                <p>We assume that \(Y\) follows a multinomial distribution with three classes that has probability function,</p>
    
                <div class="wrap">\[
                P(Y = y|\vec{x}) = p_1^{I(y=1)}.p_2^{I(y=2)}.p_3^{I(y=3)} \ \ \ \ \ \ ........ (1)
                \]</div>

                <p>where, the first two conditional probabilities are as follows,</p>
                <div class="wrap">\[
                p_1 = P(Y = 1|\vec{x}) =
                \frac{e^{\vec{x}^T.\vec{\beta}_1}}{1+e^{\vec{x}^T.\vec{\beta}_1}+e^{\vec{x}^T.\vec{\beta}_2}}, \ \
                p_2 = P(Y = 2|\vec{x}) =
                \frac{e^{\vec{x}^T.\vec{\beta}_2}}{1+e^{\vec{x}^T.\vec{\beta}_1}+e^{\vec{x}^T.\vec{\beta}_2}}
                \]</div>

                <p>As sum of the conditional probabilities for the three classes is 1 so,</p>
    
                <div class="wrap">\[
                \begin{aligned}
                p_3 = P(Y = 3|\vec{x})
                &= 1 \ - \ P(Y = 1|\vec{x}) - P(Y = 2|\vec{x}) \\
                \\
                &= 1 -
                \frac{e^{\vec{x}^T.\vec{\beta}_1}}{1+e^{\vec{x}^T.\vec{\beta}_1}+e^{\vec{x}^T.\vec{\beta}_2}}-\frac{e^{\vec{x}^T.\vec{\beta}_2}}{1+e^{\vec{x}^T.\vec{\beta}_1}+e^{\vec{x}^T.\vec{\beta}_2}}
                \\
                \\
                &= \frac{1}{1+e^{\vec{x}^T.\vec{\beta}_1}+e^{\vec{x}^T.\vec{\beta}_2}}
                \end{aligned}
                \]</div>

                <p>In the above three probabilities \(\vec{\beta}_1\) and \(\vec{\beta}_2\) are p dimensional unknown vectors and
                we have to estimate them using some parameter estimation technic.</p>
    
                <h2 id="Section2">Coefficient Estimates by MLE</h2>
                <p>In MLE we first calculate likelihood take log of it, and calculate the set of parameters for which the
                log-likelihood will be maximum. In simple terms, we will build a function using the unknown parameters, that
                function will be called likelihood, we take log of it and maximize it with respect to the parameters.</p>
    
                <h3 id="Section21">Deriving the Log-Likelihood</h3>
                <p>Suppose there are n number of observations in the data; and for each of the observations \((\vec{x}_i, y_i)\),
                \(\vec{x}_i\) is the value of the regressor vector and \(y_i\) is the observed value of the response variable.
                Think like, each of the \(y_i\)s is an observed value of a random variable with before mentioned multinomial
                distribution, ie. each of the \(y_i\)s come from a \(Y_i\) where distribution of \(Y_i\) is,
                \(P(Y_i = y_i|\vec{x}_i) = p_{1i}^{I(y_i=1)}.p_{2i}^{I(y_i=2)}.p_{3i}^{I(y_i=3)}\) where,</p>
                <div class="wrap">\[\begin{aligned}
                &p_{1i} =
                \frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}} \\
                &p_{2i} =
                \frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}} \\
                &p_{3i} =
                \frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}
                \end{aligned}
                \]</div>

                <p>Now, if you get confused from where the i comes, you can think that, for each of the n observations value of
                \(x\) is different and we are denoting the different values by \(x_i\); now as \(P(Y = y|\vec{x})\) is a
                conditional distribution on \(x\), so if \(x\) changes (\(x_1, x_2, x_3, ..., x_n\)) the conditional distribution
                changes.</p>
    
                <p>So, likelihood,</p>
                <div class="wrap">\[\begin{aligned}
                L &= \prod_{i=1}^n P(Y_i = y_i|\vec{x}_i) \\
                &= \prod_{i=1}^n p_{1i}^{I(y_i=1)}.p_{2i}^{I(y_i=2)}.p_{3i}^{I(y_i=3)} \\
                &= \prod_{i=1}^n
                \left[\frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{I(y_i=1)}\left[\frac{e^{\vec{x}_i^T.\vec{\beta}_2}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{I(y_i=2)}\left[\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{I(y_i=3)}
                \\
                &= \prod_{i=1}^n \ e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)} \ . \
                e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}.\left[\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\right]^{[I(y_i=1)
                + I(y_i=2) + I(y_i=3)]} \\
                &= \prod_{i=1}^n \ e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)} \ . \
                e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}.\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}
                \end{aligned}
                \]</div>
                <p>Last equality holds as \(I(y_i=1) + I(y_i=2) + I(y_i=3) = 1\).</p>
    
                <p>So log-likelihood,</p>
                <div class="wrap">\[\begin{aligned}
                log L &= log \ \prod_{i=1}^n \ e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)} \ . \
                e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}.\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}
                \\
                &= \sum_{i=1}^nlog\left[e^{\vec{x}_i^T.\vec{\beta}_1.I(y_i=1)}\right] +
                log\left[e^{\vec{x}_i^T.\vec{\beta}_2.I(y_i=2)}\right] -
                log\left[1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}\right] \\
                &= \sum_{i=1}^n I(y_i=1).\vec{x}_i^T.\vec{\beta}_1 + \sum_{i=1}^nI(y_i=2).\vec{x}_i^T.\vec{\beta}_2 -
                \sum_{i=1}^nlog\left[1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}\right]
                \end{aligned}\]</div>
    
    
                <h3 id="Section22">Deriving the First Derivative</h3>
                <p>First derivative,</p>
                <div class="wrap">\[\begin{aligned}\frac{\partial}{\partial \vec{\beta}}log L &= \begin{pmatrix}\frac{\partial}{\partial
                \vec{\beta}_1}log L \\
                \frac{\partial}{\partial \vec{\beta}_2}log L \end{pmatrix}
                \end{aligned}\]</div>
    
                <p>Now,</p>
                <div class="wrap">\[\begin{aligned}
                \frac{\partial}{\partial \vec{\beta_1}}log L &= \frac{\partial}{\partial \vec{\beta_1}} \left[\sum_{i=1}^n
                I(y_i=1).\vec{x}_i^T.\vec{\beta}_1 + \sum_{i=1}^nI(y_i=2).\vec{x}_i^T.\vec{\beta}_2 -
                \sum_{i=1}^nlog\left[1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}\right]\right] \\
                &= \sum_{i=1}^n I(y_i=1).\vec{x}_i \ + \ 0 \ - \
                \sum_{i=1}^n\frac{1}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}.e^{\vec{x}_i^T.\vec{\beta}_1}.\vec{x}_i
                \\
                &= \sum_{i=1}^n I(y_i=1) \ . \ \vec{x}_i \ \ - \ \ \sum_{i=1}^n P(Y_i=1|\vec{x}_i) \ . \ \vec{x}_i \\
                &= \sum_{i=1}^n \left[ \ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \ \right].\vec{x}_i
                \end{aligned}
                \]</div>

                <p>In the second line the second term is independent of \(\beta_1\), so derivative of it with respect to
                \(\beta_1\) is 0.</p>
    
                <p>Similarly,</p>
                <div class="wrap">\[\frac{\partial}{\partial \vec{\beta_2}}log L = \sum_{i=1}^n \left[ \ \ I(y_i=2) - P(Y_i=2|\vec{x}_i) \ \
                \right].\vec{x}_i\]</div>
                <p>So,</p>
                <div class="wrap">\[\begin{aligned}
                \frac{\partial}{\partial \vec{\beta}}log L &= \begin{pmatrix}\frac{\partial}{\partial \vec{\beta}_1}log L \\
                \frac{\partial}{\partial \vec{\beta}_2}log L \end{pmatrix} \\
                \\
                &= \begin{bmatrix}\sum_{i=1}^n \left[ \ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \ \right].\vec{x}_i \\
                \sum_{i=1}^n \left[ \ \ I(y_i=2) - P(Y_i=2|\vec{x}_i) \ \ \right].\vec{x}_i\end{bmatrix} \\
                \\
                &= \sum_{i=1}^n \begin{bmatrix} \left[ \ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \ \right].\vec{x}_i \\
                \left[ \ \ I(y_i=2) - P(Y_i=2|\vec{x}_i) \ \ \right].\vec{x}_i\end{bmatrix} \\
                \\
                &= \sum_{i=1}^n \begin{bmatrix}\ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \ \\
                \ \ I(y_i=2) - P(Y_i=2|\vec{x}_i) \ \ \end{bmatrix} \otimes \ \vec{x}_i
                \end{aligned}\]</div>
                <p>Here, \(\ \ \ \vec{\beta}^T = \left(\vec{\beta}_1^T \ \ \ \vec{\beta}_2^T\right)\) and \(\ \ \otimes\) is
                **Kronecker product**. Note that, the unknown parameters are hidden inside \(P(Y_i=1|\vec{x}_i)\) and
                \(P(Y_i=2|\vec{x}_i)\).</p>
    
                <h3 id="Section23">How to Solve for the Coefficients</h3>
                <p>Now to get the parameter estimates, we will solve the equation,</p> 
                <div class="wrap">\[\frac{\partial}{\partial \vec{\beta}}log
                L=\sum_{i=1}^n \begin{bmatrix}\ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \ \\
                \ \ I(y_i=2) - P(Y_i=2|\vec{x}_i) \ \ \end{bmatrix} \otimes \ \vec{x}_i = 0\]</div>
                <p>But here the problem is that, due to the complicated form of \(P(Y_i=1|\vec{x}_i)\) and \(P(Y_i=2|\vec{x}_i)\)
                we can not solve the set equations as linear regression and get a closed form of the parameter estimates, to
                get the parameter estimates we have to use numerical methods. But we will not discuss it here, we will see
                various numerical methods first and come back to it again.</p>
                <h2 id="Section3">Proving Concavity of the Problem</h2>
                <p>Now we are in a state that we have a function and we are maximizing it, but it is very common in
                maximization and minimization problem that the equation \(\frac{\partial}{\partial \vec{\beta}}log L = 0\)
                yields more than one solution, so which one to take, in that case we have to calculate value of the
                log-likelihood for each of the solution and select the one for which log-likelihood is maximum. But this is
                not the case with concave functions, concave functions have unique solution to \(\frac{\partial}{\partial
                \vec{\beta}}log L = 0\) and this is the point of maxima. Now the necessary and sufficient condition for a
                function to be concave is the **Hessian matrix** of the function is negative semi-definite. So if we can
                prove that Hessian matrix of our problem is negative semi-definite we can be sure that the solution of the
                above equation will give the point of maxima.</p>
                <h3 id="Section31">Deriving Hessian Matrix</h3>
                <p>Hessian matrix,</p>
                <div class="wrap">\[\begin{aligned}
                H &= \frac{\partial}{\partial \vec{\beta}}\left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T \\
                &= \begin{bmatrix}\underbrace{\frac{\partial}{\partial \vec{\beta}_1} \left[ \frac{\partial}{\partial
                \vec{\beta}}log \ L \right]^T}_{H_1} \\
                \underbrace{\frac{\partial}{\partial \vec{\beta}_2} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L
                \right]^T}_{H_2}
                \end{bmatrix}
                \end{aligned}
                \]</div>
                <p>Calculating the derivatives in the hessian matrix is not staright forward. So, we will derive them
                seperately.</p>
    
                <h4>Deriving H1</h4>
                <p>Now,</p>
                <div class="wrap">\[\begin{aligned}
                H_1 &=\frac{\partial}{\partial \vec{\beta}_1} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T \\
                &=\frac{\partial}{\partial \vec{\beta}_1} \sum_{i=1}^n\left[ \ I(y_i=1) - P(Y_i=1|\vec{x}_i) \ \ \ \ I(y_i=2)
                - P(Y_i=2|\vec{x}_i) \ \right] \otimes \vec{x}_i^T \\
                &= \sum_{i=1}^n\left[ \ \frac{\partial}{\partial \vec{\beta}_1}\left[I(y_i=1) - P(Y_i=1|\vec{x}_i)\right] \
                \ \ \ \ \frac{\partial}{\partial \vec{\beta}_1}\left[I(y_i=2) - P(Y_i=2|\vec{x}_i)\right] \ \right] \otimes
                \vec{x}_i^T \\
                &= \sum_{i=1}^n \left[ \ \ 0 - \frac{\partial}{\partial \vec{\beta}_1}P(Y_i=1|\vec{x}_i) \ \ \ \ 0 -
                \frac{\partial}{\partial \vec{\beta}_1}P(Y_i=2|\vec{x}_i) \ \ \right] \otimes \vec{x}_i^T \\
                &= -\sum_{i=1}^n \left[ \ \ \underbrace{\frac{\partial}{\partial \vec{\beta}_1}P(Y_i=1|\vec{x}_i)}_{H_{11}} \ \ \ \
                \underbrace{\frac{\partial}{\partial \vec{\beta}_1}P(Y_i=2|\vec{x}_i)}_{H_{12}} \ \ \right] \otimes \vec{x}_i^T
                (1)
                \end{aligned}
                \]</div>
                <p>Again two derivatives, will do it seperately.</p>
                <p>Upon calculation we get,</p>
                <div class="wrap">\[H_{11}=P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).\vec{x}_i\]</div>
                <details>
                    <summary>
                        <span>Deriving \(H_{11}\)</span>
                    </summary>
                    <p>It is always recommended to use vector matrix differentiation where differentiation with respect to a vector
                    is involved, alternatively we can calculate derivatives with respect to every element of \(\beta_1\) (ie.
                    \(\frac{\partial}{\partial \beta_{11}}P(Y_i=1|\vec{x}_i)\), \(\frac{\partial}{\partial
                    \beta_{12}}P(Y_i=1|\vec{x}_i)\), \(\frac{\partial}{\partial \beta_{13}}P(Y_i=1|\vec{x}_i)\),
                    ...,\(\frac{\partial}{\partial \beta_{1p}}P(Y_i=1|\vec{x}_i)\)) and stack them one below other to get
                    \(\frac{\partial}{\partial \vec{\beta}_1}P(Y_i=1|\vec{x}_i)\).</p>
                    <div class="wrap">\[\begin{aligned}
                    &\frac{\partial}{\partial \beta_{1j}} P(Y_i=1|\vec{x}_i) \\ = & \ \frac{\partial}{\partial \beta_{1j}}
                    \frac{e^{\vec{x}_i^T.\vec{\beta}_1}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}} \\
                    \\
                    = & \ 
                    \frac{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}).e^{\vec{x}_i^T.\vec{\beta}_1}.x_{ij} -
                    (e^{\vec{x}_i^T.\vec{\beta}_1})^2.x_{ij}}{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2})^2}
                    \\
                    \\
                    = &
                    \frac{e^{\vec{x}_i^T.\vec{\beta}_1}.x_{ij}}{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2})^2}.[1+e^{\vec{x}_i^T.\vec{\beta}_2}]
                    \\
                    \\
                    = & \ P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).x_{ij}
                    \end{aligned}\]</div>
        
                    <p>So, we know the form of each of the derivative, now we can vary j and stack the elements one below other.</p>
        
                    <div class="wrap">\[\begin{aligned}
                    H_{11}&=\frac{\partial}{\partial \vec{\beta}_1}P(Y_i=1|\vec{x}_i) \\ &=
                    \begin{pmatrix}
                    \frac{\partial}{\partial \beta_{11}} P(Y_i=1|\vec{x}_i) \\
                    \frac{\partial}{\partial \beta_{12}} P(Y_i=1|\vec{x}_i) \\
                    . \\
                    . \\
                    . \\
                    \frac{\partial}{\partial \beta_{1p}} P(Y_i=1|\vec{x}_i) \\
                    \end{pmatrix} \\
                    \\
                    &= \begin{pmatrix}P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).x_{i1} \\
                    P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).x_{i2} \\
                    . \\
                    . \\
                    . \\
                    P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).x_{ip}
                    \end{pmatrix} \\
                    \\
                    &= P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).\begin{pmatrix}x_{i1} \\
                    x_{i2} \\
                    x_{i3} \\
                    . \\
                    . \\
                    . \\
                    x_{ip}
                    \end{pmatrix} \\
                    \\
                    &= P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).\vec{x}_i
                    \end{aligned}
                    \]</div>
                </details>

                <p>and,</p>
            <div class="wrap">\[H_{12}=-P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i).\vec{x}_i\]</div>

            <details>
                <summary>
                    <span>Deriving \(H_{12}\)</span>
                </summary>
                <div class="wrap">\[P(Y_i=2|\vec{x}_i) =
                \frac{e^{\vec{x}_i^T.\vec{\beta}_2}}{1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2}}\]</div>
    
                <p>and,</p>
                <div class="wrap">\[\begin{aligned}
                \frac{\partial}{\partial \beta_{1j}} P(Y_i=2|\vec{x}_i) &= -
                \frac{e^{\vec{x}_i^T.\vec{\beta}_2}.e^{\vec{x}_i^T.\vec{\beta}_1}.x_{ij}}{(1+e^{\vec{x}_i^T.\vec{\beta}_1}+e^{\vec{x}_i^T.\vec{\beta}_2})^2}
                \\
                &= - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i).x_{ij} \\
                \end{aligned}\]</div>
    
                <p>So again if we stack the partial derivatives one below other as before, we will get the required partial
                derivative vector.</p>
                <div class="wrap">\[H_{12}=\frac{\partial}{\partial \vec{\beta}_1}P(Y_i=2|\vec{x}_i) = -
                P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i).\vec{x}_i\]</div>
            </details>


            <p>If we substitute \(H_{11}\) and \(H_{12}\) in \(H_1\) we get,</p>
            <div class="wrap">\[\begin{aligned}
            H_1&=\frac{\partial}{\partial \vec{\beta}_1}\left[\frac{\partial}{\partial \vec{\beta}}log L\right]^T \\ &=
            -\sum_{i=1}^n \left[ \ \ \frac{\partial}{\partial \vec{\beta}_1}P(Y_i=1|\vec{x}_i) \ \ \ \
            \frac{\partial}{\partial \vec{\beta}_1}P(Y_i=2|\vec{x}_i) \ \ \right] \otimes \vec{x}_i^T \\
            &= -\sum_{i=1}^n \left[P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i).\vec{x}_i \ \ \ \ -
            P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i).\vec{x}_i\right] \otimes \vec{x}_i^T \\
            &= -\sum_{i=1}^n\left[P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ -
            P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i)\right]\otimes\vec{x}_i\otimes\vec{x}_i^T \\
            &= -\sum_{i=1}^n\left[P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ -
            P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T\end{aligned}\]</div>

            <h4>Deriving H2</h4>
            <p>Similarly if we calculate the derivatives properly we can get,</p>
            <div class="wrap">\[
            \begin{aligned}
            \frac{\partial}{\partial \vec{\beta}_2}\left[\frac{\partial}{\partial \vec{\beta}}log L\right]^T =
            -\sum_{i=1}^n\left[-P(Y_i=2|\vec{x}_i)P(Y_i=1|\vec{x}_i) \ \ \ \
            P(Y_i=2|\vec{x}_i)P(Y_i\neq2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T\end{aligned}\]</div>

            <h4>Substituting H1 and H2 in Hessian Matrix</h4>
            <p>Now if we substitute \(H_1\) and \(H_2\) we get,</p>
            <div class="wrap">\[\begin{aligned}
            H &= \begin{bmatrix}\frac{\partial}{\partial \vec{\beta}_1} \left[ \frac{\partial}{\partial \vec{\beta}}log
            \ L \right]^T \\
            \frac{\partial}{\partial \vec{\beta}_2} \left[ \frac{\partial}{\partial \vec{\beta}}log \ L \right]^T
            \end{bmatrix} \\
            \\
            &= \begin{bmatrix}-\sum_{i=1}^n\left[P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ \ -
            P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T \\
            -\sum_{i=1}^n\left[-P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \ \
            P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i)\right]\otimes\vec{x}_i.\vec{x}_i^T\end{bmatrix} \\
            \\
            &= -\sum_{i=1}^n\begin{bmatrix}P(y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ -
            P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\
            -P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \ P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i)
            \end{bmatrix} \otimes \vec{x}_i.\vec{x}_i^T
            \end{aligned}
            \]</div>

            <h3 id="Section32">Proving Concavity of Hessian Matrix</h3>
            <p>To have unique maxima to the problem, \(H\) has to be negative semi-definite, or \(-H\) positive semi-definite.
            We can prove it using the following theorems.</p>
            <h4>1. Sum of positive semi-definite matrices is positive semi-definite.</h4>
            <p>-H is sum of matrices. So, if we can show that the term inside sum is positive semi-definite then -H is
            positive semi-definite; ie. we have to prove
            \(S_i=\begin{bmatrix}P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ -
            P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\ -P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \
            P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i)\end{bmatrix} \otimes \vec{x}_i.\vec{x}_i^T\) is positive
            semi-definite for each of \(i\). It can be proved using the following theorem.</p>

            <h4>2.Kronecker Product of two positive semi-definite matrices is positive semi-definite.</h4>
            <p>So the problem boils down to showing positive semi-definniteness of
            \(\begin{bmatrix}P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i)
            \\ -P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \ P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i)
            \end{bmatrix}\) and \(\vec{x}_i.\vec{x}_i^T\). It follows from the following theory:</p>

            <h4>3. For any vector \(\vec{v}\), \(\vec{v}.\vec{v}^T\) is positive semi-definite.</h4>
            <p>**Proof:** To prove it we have to show, for any \(z\neq0\) vector, \(\vec{z}^T.\vec{v}.\vec{v}^T.\vec{z}\geq\).
            Now, \(\vec{z}^T.\vec{v}.\vec{v}^T.\vec{z}=\left(\vec{z}^T.\vec{v}\right)^2\geq0\), because
            \(\vec{z}^T.\vec{v}=\vec{v}^T.\vec{z}=\) a scalar. If we replace \(v\) with \(\vec{x}_i\) we will
            get\(\vec{x}_i.\vec{x}_i^T\) is positive semi-definite.</p>

            <h4>4.Using Cauchy-Schwarz Inequality</h4>
            <p>Only thing remaining to prove, positive semi-definiteness of
            \(\begin{bmatrix}P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i)
            \\ -P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \ P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i)
            \end{bmatrix}\).</p>

            <p>Now let,</p>
            <div class="wrap">\[\begin{aligned}
            P &= \begin{bmatrix}
            P(Y_i=1|\vec{x}_i).P(Y_i\neq1|\vec{x}_i) \ \ \ \ \ - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\
            -P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) \ \ \ \ \ \ P(Y_i=2|\vec{x}_i).P(Y_i\neq2|\vec{x}_i)
            \end{bmatrix} \\
            \\
            &= \begin{bmatrix}
            P(Y_i=1|\vec{x}_i).[1-P(Y_i=1|\vec{x}_i)] & - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\
            -P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) & P(Y_i=2|\vec{x}_i).[1-P(Y_i=2|\vec{x}_i)]
            \end{bmatrix} \\
            \\
            &= \begin{bmatrix}
            P(Y_i=1|\vec{x}_i)-P^2(Y_i=1|\vec{x}_i) & - P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\
            -P(Y_i=2|\vec{x}_i).P(Y_i=1|\vec{x}_i) & P(Y_i=2|\vec{x}_i)-P^2(Y_i=2|\vec{x}_i)]
            \end{bmatrix} \\
            \\
            &= \begin{bmatrix}
            P(Y_i=1|\vec{x}_i) & 0 \\
            0 & P(Y_i=2|\vec{x}_i)
            \end{bmatrix} -
            \begin{bmatrix}
            P^2(Y_i=1|\vec{x}_i) & P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) \\
            P(Y_i=1|\vec{x}_i).P(Y_i=2|\vec{x}_i) & P^2(Y_i=2|\vec{x}_i)\end{bmatrix} \\
            \\
            &= \begin{bmatrix}
            P(Y_i=1|\vec{x}_i) & 0 \\
            0 & P(Y_i=2|\vec{x}_i)
            \end{bmatrix} -
            \begin{bmatrix}
            P(Y_i=1|\vec{x}_i) \\
            P(Y_i=2|\vec{x}_i)\end{bmatrix}.
            \begin{bmatrix}
            P(Y_i=1|\vec{x}_i) \\
            P(Y_i=2|\vec{x}_i)\end{bmatrix}^T
            \end{aligned}\]</div>

            <p>Now, to prove the positive semi-definiteness, we have to show for any \(z\neq0\), \(\vec{z}^T.P.\vec{z}\geq0\).
            So,</p>
            <div class="wrap">\[\begin{aligned}
            & \vec{z}^T.P.\vec{z} \\
            &= \vec{z}^T.\begin{bmatrix}
            P(Y_i=1|\vec{x}_i) & 0 \\
            0 & P(Y_i=2|\vec{x}_i)
            \end{bmatrix}.\vec{z} - \vec{z}^T.
            \begin{bmatrix}
            P(Y_i=1|\vec{x}_i) \\
            P(Y_i=2|\vec{x}_i)\end{bmatrix}.
            \begin{bmatrix}
            P(Y_i=1|\vec{x}_i) \\
            P(Y_i=2|\vec{x}_i)\end{bmatrix}^T.\vec{z} \\
            \\
            &= \sum_{j=1}^2P(Y_i=j|\vec{x}_i)z_j^2 \ \ - \ \ \left(\sum_{j=1}^2P(Y_i=j|\vec{x}_i)z_j\right)^2
            \end{aligned}\]</div>

            <p>Now, by **Cauchy-Schwarz Inequality**, </p>
            <div class="wrap">\[\left(\sum_{j=1}^l u_jv_j\right)^2 \ \ \leq \ \ \sum_{j=1}^l
            u_j^2.\sum_{j=1}^lv_j^2\]</div>

            <p>If we set, l =2, \(u_j=\sqrt{P(Y_i=j|\vec{x}_i)}\) also as, \(P(Y_i=j|\vec{x}_i) \geq 0\), we can set
            \(v_j=\sqrt{P(Y_i=j|\vec{x}_i)}.z_j\); we get,</p>
            <div class="wrap">\[\begin{aligned}
            &\left(\sum_{j=1}^2 \sqrt{P(Y_i=j|\vec{x}_i)}.\sqrt{P(Y_i=j|\vec{x}_i)}.z_j\right)^2 \leq
            \sum_{j=1}^2P(Y_i=j|\vec{x}_i).\sum_{j=1}^2P(Y_i=j|\vec{x}_i).z_j^2 \\
            \implies & \left(\sum_{j=1}^2 P(Y_i=j|\vec{x}_i).z_j\right)^2 \leq \sum_{j=1}^2P(Y_i=j|\vec{x}_i).z_j^2
            \end{aligned}\]</div>

            <p>As, \(\sum_{j=1}^2P(Y_i=j|\vec{x}_i) = 1\).</p>

            <p>So, \(\vec{z}^T.P.\vec{z} \geq 0\). So, P is positive semi-definite.</p>

            <p>Using the above steps we can prove that -H is negative semi-definite, so H is positive semi-definite. Which
            in term proves that, if we solve the equation \(\frac{\partial}{\partial \vec{\beta}}log L = 0\) we will
            always get one solution and the solution will be point of maxima.</p>


            <h2 id="Section4">Conclusion</h2>
            <p>In this article we have seen mainly theory behind Multionomial Logistic Regression including, choosing right
            distribution for the problem, build the set equations to get the parameter estimates using MLE also proved
            concavity of the problem. Here we are not actually solving the set of equations of MLE, the reason is, we
            cannot get closed form solution from the equations, we have to use numerical methods. As we have not
            discussed any of it we are leaving it for future articles.</p>

            <div class="navigator">
                <span><a href="Binomial_Logistic_Regression.html">&#8249;</a><span style="margin-left:5px">Binomial Logistic Regression</span></span>
                <span style="margin-right:5px"><span>PCA</span><a href="PCA.html">&#8250;</a></span>
            </div>
            </div>



















            



        </div>
    </div>


</body>

</html>