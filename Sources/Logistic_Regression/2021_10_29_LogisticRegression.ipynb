{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "2021-10-29-LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d44d29b7"
      },
      "source": [
        "# Mathematics behind Logistic Regression (Binomial)\n",
        "\n",
        "> Learn how Logistic Regression is different from Linear Regression; formulate the problem, dervie the Hessian matrix and show that the problem is negative definite.\n",
        "\n",
        "- toc:true\n",
        "- badges:true\n",
        "- comments:true\n",
        "- categories:[Logistic Regression, MLE, Hessian Matrix]\n",
        "- image: images/Logistic_Regression.png"
      ],
      "id": "d44d29b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c8e632e"
      },
      "source": [
        "Let $\\vec{x} = (x_1, x_2, x_3, ..., x_p)^T$ is the p dimensional independent vector and $y$ is the categorical dependent variable. For now suppose $y$ has only two categories (for more than two categories mathematics is little different). We will always represent categories by 0 and 1. In some cases to do that we may have to use appropriate technic to represent categories by 0 and 1. For example, if categories are boy or girl then we can represent 0 for boy and 1 for girl, alternatively 0 for girl and 1 for boy.\n",
        "\n",
        "## Mathematical Representation\n",
        "Now,we may try to fit a linear regression model of the form,\n",
        "$y_i=\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + ... + \\beta_p x_{ip} + \\epsilon_i$\n",
        "then, as all $y$s are not same, mix of 0 and 1, all the coefficient estimates can not be zero at the same time; ie. the regression plane would have non-zero slope along some axis. So, for suitable choice of $x$ vector predicted value of y would be greater than 0, similarly for suitable choice of x predicted value of y can be negative (becomes unbounded briefly said). So, what we do is, we set,\n",
        "$$y_i \\sim Bernoulli(p_i)$$,\n",
        "where, $$p_i = P(y_i = 1) = \\frac{1}{1+ exp[-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3} + ... + \\beta_p x_{ip})]}$$\n",
        "and estimate the parameters by **MLE**.\n",
        "\n",
        "## Coefficient Estimates by MLE\n",
        "### Form of the Log Likelihood\n",
        "Likelihood,\n",
        "$$\\begin{aligned}\n",
        "L &= \\prod_{i=1}^nP(y_i) \\\\\n",
        "  &= \\prod_{i=1}^np_i^{y_i}(1 - p_i)^{1 - y_i} \n",
        "\\end{aligned}$$\n",
        "\n",
        "So, log-likelihood,\n",
        "$$\\begin{aligned}\n",
        "l &= \\log L \\\\\n",
        "  &= \\log \\prod_{i=1}^np_i^{y_i}(1 - p_i)^{1 - y_i} \\\\\n",
        "  &= \\sum_{i=1}^n y_i\\log p_i + (1-y_i)\\log(1-p_i) \\\\\n",
        "  &= \\sum_{i=1}^n y_i.\\log \\frac{1}{1+ exp[-\\vec{\\beta}^T.\\vec{x}_i]} + (1-y_i).\\log \\frac{exp[-\\vec{\\beta}^T.\\vec{x}_i]}{1+ exp[-\\vec{\\beta}^T.\\vec{x}_i]} \\\\\n",
        "  &= \\sum_{i=1}^n -y_i.\\log (1+ exp[-\\vec{\\beta}^T.\\vec{x}_i]) + (1-y_i).\\log exp[-\\vec{\\beta}^T.\\vec{x}_i] \\ - (1-y_i).\\log (1+ exp[-\\vec{\\beta}^T.\\vec{x}_i])\\\\\n",
        "  &= \\sum_{i=1}^n -y_i.\\log (1+ exp[-\\vec{\\beta}^T.\\vec{x}_i]) - (1-y_i).\\vec{\\beta}^T.\\vec{x}_i \\ - (1-y_i).\\log (1+ exp[-\\vec{\\beta}^T.\\vec{x}_i])\\\\\n",
        "  &= \\sum_{i=1}^n - (1-y_i).\\vec{\\beta}^T.\\vec{x}_i \\ - \\log (1+ exp[-\\vec{\\beta}^T.\\vec{x}_i])\\\\\n",
        "  &= -\\sum_{i=1}^n (1-y_i).\\vec{\\beta}^T.\\vec{x}_i \\ + \\log (1+ exp[-\\vec{\\beta}^T.\\vec{x}_i])\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We will get the solution by solving $\\frac{\\partial l}{\\partial \\vec{\\beta}} = 0$; \n",
        "\n",
        "### Deriving the First Derivative\n",
        "Now,\n",
        "$$\\begin{aligned}\n",
        "\\frac{\\partial l}{\\partial \\vec{\\beta}} &= \\frac{\\partial}{\\partial \\vec{\\beta}} -\\sum_{i=1}^n (1-y_i).\\vec{\\beta}^T.\\vec{x}_i \\ + \\log (1+ exp[-\\vec{\\beta}^T.\\vec{x}_i]) \\\\\n",
        " &= -\\sum_{i=1}^n (1-y_i).\\frac{\\partial}{\\partial \\vec{\\beta}} \\vec{\\beta}^T.\\vec{x}_i \\ + \\frac{\\partial}{\\partial \\vec{\\beta}}\\log (1+ exp[-\\vec{\\beta}^T.\\vec{x}_i]) \\\\\n",
        " &= -\\sum_{i=1}^n (1-y_i).\\vec{x}_i \\ + \\frac{1}{1+ exp[-\\vec{\\beta}^T.\\vec{x}_i]}.exp[-\\vec{\\beta}^T.\\vec{x}_i].(-1).\\vec{x}_i \\\\\n",
        " &= -\\sum_{i=1}^n (1-y_i).\\vec{x}_i \\ + \\sum_{i=1}^n \\frac{exp[-\\vec{\\beta}^T.\\vec{x}_i]}{1+ exp[-\\vec{\\beta}^T.\\vec{x}_i]}.\\vec{x}_i \\\\\n",
        " &= -\\sum_{i=1}^n (1-y_i).\\vec{x}_i \\ + \\sum_{i=1}^n \\frac{1}{1+ exp[\\vec{\\beta}^T.\\vec{x}_i]}.\\vec{x}_i\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Now, $\\frac{\\partial l}{\\partial \\beta} = 0$ will produce global maximum, if we prove that $l$ is concave function with respect to the model coefficients, ie. the **Hessian Matrix** is negative definite.\n",
        "$$H = \\begin{pmatrix}\\begin{pmatrix}\\frac{\\partial^2 l}{\\partial \\beta_n \\beta_m}\\end{pmatrix}\\end{pmatrix} = \\frac{\\partial }{\\partial \\vec{\\beta}}\\left[\\frac{\\partial l}{\\partial \\vec{\\beta}}\\right]^T < 0$$\n",
        "\n",
        "\n",
        "### Deriving the Hessian Matrix\n",
        "Here,\n",
        "$$\\begin{aligned}\n",
        "H &= \\frac{\\partial }{\\partial \\vec{\\beta}}\\left[\\frac{\\partial l}{\\partial \\vec{\\beta}}\\right]^T \\\\\n",
        " &= \\frac{\\partial }{\\partial \\vec{\\beta}} \\left[-\\sum_{i=1}^n (1-y_i).\\vec{x}_i \\ + \\sum_{i=1}^n \\frac{1}{1+ exp[\\vec{\\beta}^T.\\vec{x}_i]}.\\vec{x}_i \\right]^T \\\\\n",
        " &= -\\frac{\\partial }{\\partial \\vec{\\beta}} \\sum_{i=1}^n (1-y_i).\\vec{x}_i^T  \\ + \\frac{\\partial }{\\partial \\vec{\\beta}}\\sum_{i=1}^n \\frac{1}{1+ exp[\\vec{\\beta}^T.\\vec{x}_i]}.\\vec{x}_i^T \\\\\n",
        " &= 0 \\ \\ + \\sum_{i=1}^n \\left[\\frac{\\partial }{\\partial \\vec{\\beta}} \\frac{1}{1+ exp[\\vec{\\beta}^T.\\vec{x}_i]}\\right].\\vec{x}_i^T \\\\\n",
        " &= \\sum_{i=1}^n \\left[ \\frac{-1}{\\left(1+ exp[\\vec{\\beta}^T.\\vec{x}_i]\\right)^2}.exp[\\vec{\\beta}^T.\\vec{x}_i].\\vec{x}_i\\right]\\vec{x}_i^T \\\\\n",
        " &= -\\sum_{i=1}^n \\frac{1}{1+ exp[\\vec{\\beta}^T.\\vec{x}_i]}.\\frac{exp[\\vec{\\beta}^T.\\vec{x}_i]}{1+ exp[\\vec{\\beta}^T.\\vec{x}_i]}.\\vec{x}_i.\\vec{x}_i^T \\\\\n",
        " &= -\\sum_{i=1}^n \\frac{exp[-\\vec{\\beta}^T.\\vec{x}_i]}{1+ exp[-\\vec{\\beta}^T.\\vec{x}_i]}.\\frac{1}{1+ exp[-\\vec{\\beta}^T.\\vec{x}_i]}.\\vec{x}_i.\\vec{x}_i^T \\\\\n",
        " &= -\\sum_{i=1}^n p_i.(1-p_i).\\vec{x}_i.\\vec{x}_i^T \\\\\n",
        " &= -\\sum_{i=1}^n \\sqrt{p_i.(1-p_i)}.\\vec{x}_i.\\sqrt{p_i.(1-p_i)}.\\vec{x}_i^T \\\\\n",
        " &= -\\sum_{i=1}^n \\vec{x^*}_i .\\vec{x^*}_i^T \\ \\ \\leq \\ \\ 0\n",
        "\\end{aligned}$$\n",
        "\n",
        "\n",
        "\n",
        "Here, $\\vec{x^*}_i = \\sqrt{p_i.(1-p_i)}.\\vec{x}_i$. The last equality holds for the fact that $\\vec{x^*}_i .\\vec{x^*}_i^T$ is positive definite and sum of positive definite metrices is positive definite.\n",
        "\n",
        "Now if we set $\\frac{\\partial l}{\\partial \\beta_m} = 0$, we cannot get explicit form of $\\beta$ due the complicated form of $\\frac{\\partial l}{\\partial \\beta_m}$, so to get the solution we have to use numerical methods."
      ],
      "id": "8c8e632e"
    }
  ]
}