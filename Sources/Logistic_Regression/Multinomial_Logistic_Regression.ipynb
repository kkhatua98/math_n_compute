{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6yBntbwezbkJ",
   "metadata": {
    "id": "6yBntbwezbkJ"
   },
   "source": [
    "## Mathematics Behind Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fec292",
   "metadata": {
    "id": "96fec292"
   },
   "source": [
    "# Formulating the Problem\n",
    "Suppose, a p dimensional vector $x$ is our explanatory variable and $Y$ is the dependent variable. Now unlike Binomial logistic regression where there were two independent classes in $Y$, here the number of classes is more than 2. For better explanation without doing much calculation we are assuming that, there are 3 classes in the response variable $Y$. Let us denote the classes by $Y$ = 1, $Y$ = 2 and $Y$ = 3.\n",
    "\n",
    "We assume that $Y$ follows a multinomial distribution with three classes that has probability function,\n",
    "\n",
    "$$\n",
    "P(Y = y|\\vec{x}) = p_1^{I(y=1)}.p_2^{I(y=2)}.p_3^{I(y=3)} \\ \\ \\ \\ \\ \\ ........ (1)\n",
    "$$\n",
    "\n",
    "where, the first two conditional probabilities are as follows,\n",
    "$$\n",
    "p_1 = P(Y = 1|\\vec{x}) = \\frac{e^{\\vec{x}^T.\\vec{\\beta}_1}}{1+e^{\\vec{x}^T.\\vec{\\beta}_1}+e^{\\vec{x}^T.\\vec{\\beta}_2}}, \\ \\ \n",
    "p_2 = P(Y = 2|\\vec{x}) = \\frac{e^{\\vec{x}^T.\\vec{\\beta}_2}}{1+e^{\\vec{x}^T.\\vec{\\beta}_1}+e^{\\vec{x}^T.\\vec{\\beta}_2}} \n",
    "$$\n",
    "\n",
    "\n",
    "As sum of the conditional probabilities for the three classes is 1 so,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_3 = P(Y = 3|\\vec{x}) \n",
    " &= 1 \\ - \\ P(Y = 1|\\vec{x}) - P(Y = 2|\\vec{x}) \\\\\n",
    " \\\\\n",
    " &= 1 - \\frac{e^{\\vec{x}^T.\\vec{\\beta}_1}}{1+e^{\\vec{x}^T.\\vec{\\beta}_1}+e^{\\vec{x}^T.\\vec{\\beta}_2}}-\\frac{e^{\\vec{x}^T.\\vec{\\beta}_2}}{1+e^{\\vec{x}^T.\\vec{\\beta}_1}+e^{\\vec{x}^T.\\vec{\\beta}_2}} \\\\\n",
    " \\\\\n",
    " &= \\frac{1}{1+e^{\\vec{x}^T.\\vec{\\beta}_1}+e^{\\vec{x}^T.\\vec{\\beta}_2}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the above three probabilities $\\vec{\\beta}_1$ and $\\vec{\\beta}_2$ are p dimensional unknown vectors and we have to estimate them using some parameter estimation technic.\n",
    "\n",
    "# Parameter Estimation using MLE\n",
    "In MLE we first calculate likelihood take log of it, and calculate the set of parameters for which the log-likelihood will be maximum. In simple terms, we will build a function using the unknown parameters, that function will be called likelihood, we take log of it and maximize it with respect to the parameters.\n",
    "\n",
    "## Deriving the Log-Likelihood\n",
    "Suppose there are n number of observations in the data; and for each of the observations $(\\vec{x}_i, y_i)$, $\\vec{x}_i$ is the value of the regressor vector and $y_i$ is the observed value of the response variable. Think like, each of the $y_i$s is an observed value of a random variable with before mentioned multinomial distribution, ie. each of the $y_i$s come from a $Y_i$ where distribution of $Y_i$ is,\n",
    "$P(Y_i = y_i|\\vec{x}_i) = p_{1i}^{I(y_i=1)}.p_{2i}^{I(y_i=2)}.p_{3i}^{I(y_i=3)}$ where,\n",
    "$$\\begin{aligned}\n",
    "&p_{1i} = \\frac{e^{\\vec{x}_i^T.\\vec{\\beta}_1}}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}} \\\\\n",
    "&p_{2i} = \\frac{e^{\\vec{x}_i^T.\\vec{\\beta}_1}}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}} \\\\\n",
    "&p_{3i} = \\frac{e^{\\vec{x}_i^T.\\vec{\\beta}_1}}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now, if you get confused from where the i comes, you can think that, for each of the n observations value of $x$ is different and we are denoting the different values by $x_i$; now as $P(Y = y|\\vec{x})$ is a conditional distribution on $x$, so if $x$ changes ($x_1, x_2, x_3, ..., x_n$) the conditional distribution changes.\n",
    "\n",
    "So, likelihood,\n",
    "$$\\begin{aligned}\n",
    "L &= \\prod_{i=1}^n P(Y_i = y_i|\\vec{x}_i) \\\\\n",
    " &= \\prod_{i=1}^n p_{1i}^{I(y_i=1)}.p_{2i}^{I(y_i=2)}.p_{3i}^{I(y_i=3)} \\\\\n",
    " &= \\prod_{i=1}^n \\left[\\frac{e^{\\vec{x}_i^T.\\vec{\\beta}_1}}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}}\\right]^{I(y_i=1)}\\left[\\frac{e^{\\vec{x}_i^T.\\vec{\\beta}_2}}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}}\\right]^{I(y_i=2)}\\left[\\frac{1}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}}\\right]^{I(y_i=3)} \\\\\n",
    " &= \\prod_{i=1}^n \\ e^{\\vec{x}_i^T.\\vec{\\beta}_1.I(y_i=1)} \\ . \\ e^{\\vec{x}_i^T.\\vec{\\beta}_2.I(y_i=2)}.\\left[\\frac{1}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}}\\right]^{[I(y_i=1) + I(y_i=2) + I(y_i=3)]} \\\\\n",
    " &= \\prod_{i=1}^n \\ e^{\\vec{x}_i^T.\\vec{\\beta}_1.I(y_i=1)} \\ . \\ e^{\\vec{x}_i^T.\\vec{\\beta}_2.I(y_i=2)}.\\frac{1}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Last equality holds as $I(y_i=1) + I(y_i=2) + I(y_i=3) = 1$.\n",
    "\n",
    "So log-likelihood,\n",
    "$$\\begin{aligned}\n",
    "log L &= log \\ \\prod_{i=1}^n \\ e^{\\vec{x}_i^T.\\vec{\\beta}_1.I(y_i=1)} \\ . \\ e^{\\vec{x}_i^T.\\vec{\\beta}_2.I(y_i=2)}.\\frac{1}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}} \\\\\n",
    " &= \\sum_{i=1}^nlog\\left[e^{\\vec{x}_i^T.\\vec{\\beta}_1.I(y_i=1)}\\right] + log\\left[e^{\\vec{x}_i^T.\\vec{\\beta}_2.I(y_i=2)}\\right] - log\\left[1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}\\right] \\\\\n",
    " &= \\sum_{i=1}^n I(y_i=1).\\vec{x}_i^T.\\vec{\\beta}_1 + \\sum_{i=1}^nI(y_i=2).\\vec{x}_i^T.\\vec{\\beta}_2 - \\sum_{i=1}^nlog\\left[1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}\\right]\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "## Deriving the First Derivative\n",
    "First derivative,\n",
    "$$\\begin{aligned}\\frac{\\partial}{\\partial \\vec{\\beta}}log L &= \\begin{pmatrix}\\frac{\\partial}{\\partial \\vec{\\beta}_1}log L \\\\ \n",
    "\\frac{\\partial}{\\partial \\vec{\\beta}_2}log L \\end{pmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now,\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\vec{\\beta_1}}log L &= \\frac{\\partial}{\\partial \\vec{\\beta_1}} \\left[\\sum_{i=1}^n I(y_i=1).\\vec{x}_i^T.\\vec{\\beta}_1 + \\sum_{i=1}^nI(y_i=2).\\vec{x}_i^T.\\vec{\\beta}_2 - \\sum_{i=1}^nlog\\left[1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}\\right]\\right] \\\\ \n",
    " &= \\sum_{i=1}^n I(y_i=1).\\vec{x}_i \\ + \\ 0  \\ - \\ \\sum_{i=1}^n\\frac{1}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}}.e^{\\vec{x}_i^T.\\vec{\\beta}_1}.\\vec{x}_i \\\\ \n",
    " &= \\sum_{i=1}^n I(y_i=1) \\ . \\ \\vec{x}_i \\ \\ - \\ \\ \\sum_{i=1}^n P(Y_i=1|\\vec{x}_i) \\ . \\ \\vec{x}_i \\\\ \n",
    " &= \\sum_{i=1}^n \\left[ \\ \\ I(y_i=1) - P(Y_i=1|\\vec{x}_i) \\ \\ \\right].\\vec{x}_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the second line the second term is independent of $\\beta_1$, so derivative of it with respect to $\\beta_1$ is 0.\n",
    "\n",
    "Similarly,\n",
    "$$\\frac{\\partial}{\\partial \\vec{\\beta_2}}log L = \\sum_{i=1}^n \\left[ \\ \\ I(y_i=2) - P(Y_i=2|\\vec{x}_i) \\ \\ \\right].\\vec{x}_i$$\n",
    "\n",
    "So,\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\vec{\\beta}}log L &= \\begin{pmatrix}\\frac{\\partial}{\\partial \\vec{\\beta}_1}log L \\\\ \n",
    "\\frac{\\partial}{\\partial \\vec{\\beta}_2}log L \\end{pmatrix} \\\\ \n",
    " \\\\\n",
    " &= \\begin{bmatrix}\\sum_{i=1}^n \\left[ \\ \\ I(y_i=1) - P(Y_i=1|\\vec{x}_i) \\ \\ \\right].\\vec{x}_i \\\\ \n",
    " \\sum_{i=1}^n \\left[ \\ \\ I(y_i=2) - P(Y_i=2|\\vec{x}_i) \\ \\ \\right].\\vec{x}_i\\end{bmatrix} \\\\ \n",
    " \\\\ \n",
    " &= \\sum_{i=1}^n \\begin{bmatrix} \\left[ \\ \\ I(y_i=1) - P(Y_i=1|\\vec{x}_i) \\ \\ \\right].\\vec{x}_i \\\\ \n",
    " \\left[ \\ \\ I(y_i=2) - P(Y_i=2|\\vec{x}_i) \\ \\ \\right].\\vec{x}_i\\end{bmatrix} \\\\ \n",
    " \\\\ \n",
    " &= \\sum_{i=1}^n \\begin{bmatrix}\\ \\ I(y_i=1) - P(Y_i=1|\\vec{x}_i) \\ \\  \\\\ \n",
    " \\ \\ I(y_i=2) - P(Y_i=2|\\vec{x}_i) \\ \\ \\end{bmatrix} \\otimes \\ \\vec{x}_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "Here, $\\ \\ \\ \\vec{\\beta}^T = \\left(\\vec{\\beta}_1^T \\ \\ \\ \\vec{\\beta}_2^T\\right)$ and $\\ \\ \\otimes$ is **Kronecker product**. Note that, the unknown parameters are hidden inside $P(Y_i=1|\\vec{x}_i)$ and $P(Y_i=2|\\vec{x}_i)$.\n",
    "\n",
    "## Estimating the Parameters\n",
    "Now to get the parameter estimates, we will solve the equation, $$\\frac{\\partial}{\\partial \\vec{\\beta}}log L=\\sum_{i=1}^n \\begin{bmatrix}\\ \\ I(y_i=1) - P(Y_i=1|\\vec{x}_i) \\ \\  \\\\ \n",
    " \\ \\ I(y_i=2) - P(Y_i=2|\\vec{x}_i) \\ \\ \\end{bmatrix} \\otimes \\ \\vec{x}_i = 0$$\n",
    "\n",
    "But here the problem is that, due to the complicated form of $P(Y_i=1|\\vec{x}_i)$ and $P(Y_i=2|\\vec{x}_i)$ we can not solve the set equations as linear regression and get a closed form of the parameter estimates, to get the parameter estimates we have to use numerical methods. But we will not discuss it here, we will see various numerical methods first and come back to it again.\n",
    "\n",
    "\n",
    "\n",
    "# Proving Concavity of the Problem\n",
    "Now we are in a state that we have a function and we are maximizing it, but it is very common in maximization and minimization problem that the equation $\\frac{\\partial}{\\partial \\vec{\\beta}}log L = 0$ yields more than one solution, so which one to take, in that case we have to calculate value of the log-likelihood for each of the solution and select the one for which log-likelihood is maximum. But this is not the case with concave functions, concave functions have unique solution to $\\frac{\\partial}{\\partial \\vec{\\beta}}log L = 0$ and this is the point of maxima. Now the necessary and sufficient condition for a function to be concave is the **Hessian matrix** of the function is negative semi-definite. So if we can prove that Hessian matrix of our problem is negative semi-definite we can be sure that the solution of the above equation will give the point of maxima.\n",
    "\n",
    "## Deriving Hessian Matrix\n",
    "Hessian matrix, \n",
    "$$\\begin{aligned}\n",
    "H &= \\frac{\\partial}{\\partial \\vec{\\beta}}\\left[ \\frac{\\partial}{\\partial \\vec{\\beta}}log \\ L \\right]^T \\\\\n",
    " &= \\begin{bmatrix}\\underbrace{\\frac{\\partial}{\\partial \\vec{\\beta}_1} \\left[ \\frac{\\partial}{\\partial \\vec{\\beta}}log \\ L \\right]^T}_{H_1} \\\\ \n",
    "    \\underbrace{\\frac{\\partial}{\\partial \\vec{\\beta}_2} \\left[ \\frac{\\partial}{\\partial \\vec{\\beta}}log \\ L \\right]^T}_{H_2}\n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Calculating the derivatives in the hessian matrix is not staright forward. So, we will derive them seperately.\n",
    "\n",
    "### Deriving H1\n",
    "Now,\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\vec{\\beta}_1} \\left[ \\frac{\\partial}{\\partial \\vec{\\beta}}log \\ L \\right]^T &= \\frac{\\partial}{\\partial \\vec{\\beta}_1} \\sum_{i=1}^n\\left[ \\ I(y_i=1) - P(Y_i=1|\\vec{x}_i) \\ \\ \\ \\ I(y_i=2) - P(Y_i=2|\\vec{x}_i) \\ \\right] \\otimes \\vec{x}_i^T \\\\ \n",
    " &= \\sum_{i=1}^n\\left[ \\ \\frac{\\partial}{\\partial \\vec{\\beta}_1}\\left[I(y_i=1) - P(Y_i=1|\\vec{x}_i)\\right] \\ \\ \\ \\ \\ \\frac{\\partial}{\\partial \\vec{\\beta}_1}\\left[I(y_i=2) - P(Y_i=2|\\vec{x}_i)\\right] \\ \\right] \\otimes \\vec{x}_i^T \\\\\n",
    " &= \\sum_{i=1}^n \\left[ \\ \\ 0 - \\frac{\\partial}{\\partial \\vec{\\beta}_1}P(Y_i=1|\\vec{x}_i) \\ \\ \\ \\  0 - \\frac{\\partial}{\\partial \\vec{\\beta}_1}P(Y_i=2|\\vec{x}_i) \\ \\ \\right] \\otimes \\vec{x}_i^T \\\\ \n",
    " &= -\\sum_{i=1}^n \\left[ \\ \\ \\underbrace{\\frac{\\partial}{\\partial \\vec{\\beta}_1}P(Y_i=1|\\vec{x}_i)}_* \\ \\ \\ \\  \\underbrace{\\frac{\\partial}{\\partial \\vec{\\beta}_1}P(Y_i=2|\\vec{x}_i)}_{**} \\ \\ \\right] \\otimes \\vec{x}_i^T &(1)\n",
    "\\end{aligned} \n",
    "$$\n",
    " \n",
    "Again two derivatives, will do it seperately.\n",
    "\n",
    "#### Deriving  *\n",
    "It is always recommended to use vector matrix differentiation where differentiation with respect to a vector is involved, alternatively we can calculate derivatives with respect to every element of $\\beta_1$ (ie. $\\frac{\\partial}{\\partial \\beta_{11}}P(Y_i=1|\\vec{x}_i)$, $\\frac{\\partial}{\\partial \\beta_{12}}P(Y_i=1|\\vec{x}_i)$, $\\frac{\\partial}{\\partial \\beta_{13}}P(Y_i=1|\\vec{x}_i)$, ...,$\\frac{\\partial}{\\partial \\beta_{1p}}P(Y_i=1|\\vec{x}_i)$) and stack them one below other to get $\\frac{\\partial}{\\partial \\vec{\\beta}_1}P(Y_i=1|\\vec{x}_i)$.\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\beta_{1j}} P(Y_i=1|\\vec{x}_i) &= \\frac{\\partial}{\\partial \\beta_{1j}} \\frac{e^{\\vec{x}_i^T.\\vec{\\beta}_1}}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}} \\\\ \n",
    " \\\\\n",
    " &= \\frac{(1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}).e^{\\vec{x}_i^T.\\vec{\\beta}_1}.x_{ij} - (e^{\\vec{x}_i^T.\\vec{\\beta}_1})^2.x_{ij}}{(1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2})^2} \\\\ \n",
    " \\\\\n",
    " &= \\frac{e^{\\vec{x}_i^T.\\vec{\\beta}_1}.x_{ij}}{(1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2})^2}.[1+e^{\\vec{x}_i^T.\\vec{\\beta}_2}] \\\\ \n",
    " \\\\\n",
    " &= P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i).x_{ij}\n",
    "\\end{aligned}$$\n",
    "\n",
    "So, we know the form of each of the derivative, now we can vary j and stack the elements one below other.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\vec{\\beta}_1}P(Y_i=1|\\vec{x}_i) &= \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial}{\\partial \\beta_{11}} P(Y_i=1|\\vec{x}_i) \\\\ \n",
    "\\frac{\\partial}{\\partial \\beta_{12}} P(Y_i=1|\\vec{x}_i) \\\\ \n",
    ". \\\\ \n",
    ". \\\\ \n",
    ". \\\\ \n",
    "\\frac{\\partial}{\\partial \\beta_{1p}} P(Y_i=1|\\vec{x}_i) \\\\ \n",
    "\\end{pmatrix} \\\\ \n",
    " \\\\ \n",
    " &= \\begin{pmatrix}P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i).x_{i1} \\\\ \n",
    " P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i).x_{i2} \\\\ \n",
    " . \\\\ \n",
    " . \\\\ \n",
    " . \\\\ \n",
    " P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i).x_{ip} \n",
    " \\end{pmatrix} \\\\ \n",
    " \\\\ \n",
    " &= P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i).\\begin{pmatrix}x_{i1} \\\\ \n",
    " x_{i2} \\\\ \n",
    " x_{i3} \\\\ \n",
    " . \\\\ \n",
    " . \\\\ \n",
    " . \\\\ \n",
    " x_{ip}\n",
    " \\end{pmatrix} \\\\ \n",
    " \\\\ \n",
    " &= P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i).\\vec{x}_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Deriving  **\n",
    "$$P(Y_i=2|\\vec{x}_i) = \\frac{e^{\\vec{x}_i^T.\\vec{\\beta}_2}}{1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2}}$$\n",
    "\n",
    "and,\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\beta_{1j}} P(Y_i=2|\\vec{x}_i) &= - \\frac{e^{\\vec{x}_i^T.\\vec{\\beta}_2}.e^{\\vec{x}_i^T.\\vec{\\beta}_1}.x_{ij}}{(1+e^{\\vec{x}_i^T.\\vec{\\beta}_1}+e^{\\vec{x}_i^T.\\vec{\\beta}_2})^2} \\\\ \n",
    " &= - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i).x_{ij} \\\\ \n",
    "\\end{aligned}$$\n",
    "\n",
    "So again if we stack the partial derivatives one below other as before, we will get the required partial derivative vector.\n",
    "$$\\frac{\\partial}{\\partial \\vec{\\beta}_1}P(Y_i=2|\\vec{x}_i) = - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i).\\vec{x}_i$$\n",
    "\n",
    "#### Substituting * and ** in (1)\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\vec{\\beta}_1}\\left[\\frac{\\partial}{\\partial \\vec{\\beta}}log L\\right]^T &= -\\sum_{i=1}^n \\left[ \\ \\ \\frac{\\partial}{\\partial \\vec{\\beta}_1}P(Y_i=1|\\vec{x}_i) \\ \\ \\ \\  \\frac{\\partial}{\\partial \\vec{\\beta}_1}P(Y_i=2|\\vec{x}_i) \\ \\ \\right] \\otimes \\vec{x}_i^T \\\\ \n",
    " &= -\\sum_{i=1}^n \\left[P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i).\\vec{x}_i \\ \\ \\ \\  - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i).\\vec{x}_i\\right] \\otimes \\vec{x}_i^T \\\\ \n",
    " &= -\\sum_{i=1}^n\\left[P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i) \\ \\ \\ \\ - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i)\\right]\\otimes\\vec{x}_i\\otimes\\vec{x}_i^T \\\\ \n",
    " &= -\\sum_{i=1}^n\\left[P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i) \\ \\ \\ \\ - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i)\\right]\\otimes\\vec{x}_i.\\vec{x}_i^T\\end{aligned}$$\n",
    "\n",
    "### Deriving H2\n",
    "Similarly if we calculate the derivatives properly we can get,\n",
    " $$\n",
    " \\begin{aligned}\n",
    " \\frac{\\partial}{\\partial \\vec{\\beta}_2}\\left[\\frac{\\partial}{\\partial \\vec{\\beta}}log L\\right]^T = -\\sum_{i=1}^n\\left[-P(Y_i=2|\\vec{x}_i)P(Y_i=1|\\vec{x}_i) \\ \\ \\ \\ P(Y_i=2|\\vec{x}_i)P(Y_i\\neq2|\\vec{x}_i)\\right]\\otimes\\vec{x}_i.\\vec{x}_i^T\\end{aligned}$$\n",
    "\n",
    "### Substituting H1 and H2 in Hessian Matrix\n",
    " $$\\begin{aligned}\n",
    " H &= \\begin{bmatrix}\\frac{\\partial}{\\partial \\vec{\\beta}_1} \\left[ \\frac{\\partial}{\\partial \\vec{\\beta}}log \\ L \\right]^T \\\\ \n",
    "    \\frac{\\partial}{\\partial \\vec{\\beta}_2} \\left[ \\frac{\\partial}{\\partial \\vec{\\beta}}log \\ L \\right]^T\n",
    "    \\end{bmatrix} \\\\ \n",
    "  \\\\ \n",
    "  &= \\begin{bmatrix}-\\sum_{i=1}^n\\left[P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i) \\ \\ \\ \\ \\ \\ - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i)\\right]\\otimes\\vec{x}_i.\\vec{x}_i^T \\\\ \n",
    "  -\\sum_{i=1}^n\\left[-P(Y_i=2|\\vec{x}_i).P(Y_i=1|\\vec{x}_i) \\ \\ \\ \\ \\ \\ \\ P(Y_i=2|\\vec{x}_i).P(Y_i\\neq2|\\vec{x}_i)\\right]\\otimes\\vec{x}_i.\\vec{x}_i^T\\end{bmatrix} \\\\ \n",
    "  \\\\ \n",
    "  &= -\\sum_{i=1}^n\\begin{bmatrix}P(y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i) \\ \\ \\ \\ \\ - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i) \\\\ \n",
    "  -P(Y_i=2|\\vec{x}_i).P(Y_i=1|\\vec{x}_i) \\ \\ \\ \\ \\ \\ P(Y_i=2|\\vec{x}_i).P(Y_i\\neq2|\\vec{x}_i)\n",
    "  \\end{bmatrix} \\otimes \\vec{x}_i.\\vec{x}_i^T\n",
    " \\end{aligned}\n",
    " $$\n",
    "\n",
    "## Proving Concavity of Hessian Matrix\n",
    "To have unique maxima to the problem, $H$ has to be negative semi-definite, or $-H$ positive semi-definite. We can prove it using the following theorems.\n",
    "### 1. Sum of positive semi-definite matrices is positive semi-definite.\n",
    "-H is sum of matrices. So, if we can show that the term inside sum is positive semi-definite then -H is positive semi-definite; ie. we have to prove \n",
    "$S_i=\\begin{bmatrix}P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i) \\ \\ \\ \\ \\ - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i) \\\\ -P(Y_i=2|\\vec{x}_i).P(Y_i=1|\\vec{x}_i) \\ \\ \\ \\ \\ \\ P(Y_i=2|\\vec{x}_i).P(Y_i\\neq2|\\vec{x}_i)\\end{bmatrix} \\otimes \\vec{x}_i.\\vec{x}_i^T$ is positive semi-definite for each of $i$. It can be  proved using the following theorem.\n",
    "  \n",
    "### 2.Kronecker Product of two positive semi-definite matrices is positive semi-definite. \n",
    "So the problem boils down to showing positive semi-definniteness of $\\begin{bmatrix}P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i) \\ \\ \\ \\ \\ - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i) \\\\ -P(Y_i=2|\\vec{x}_i).P(Y_i=1|\\vec{x}_i) \\ \\ \\ \\ \\ \\ P(Y_i=2|\\vec{x}_i).P(Y_i\\neq2|\\vec{x}_i) \\end{bmatrix}$ and $\\vec{x}_i.\\vec{x}_i^T$. It follows from the following theory:  \n",
    "\n",
    "### 3. For any vector $\\vec{v}$, $\\vec{v}.\\vec{v}^T$ is positive semi-definite. \n",
    "**Proof:** To prove it we have to show, for any $z\\neq0$ vector, $\\vec{z}^T.\\vec{v}.\\vec{v}^T.\\vec{z}\\geq$. Now, $\\vec{z}^T.\\vec{v}.\\vec{v}^T.\\vec{z}=\\left(\\vec{z}^T.\\vec{v}\\right)^2\\geq0$, because $\\vec{z}^T.\\vec{v}=\\vec{v}^T.\\vec{z}=$ a scalar. If we replace $v$ with $\\vec{x}_i$ we will get$\\vec{x}_i.\\vec{x}_i^T$ is positive semi-definite.\n",
    "\n",
    "### 4.Using Cauchy-Schwarz Inequality\n",
    "Only thing remaining to prove, positive semi-definiteness of $\\begin{bmatrix}P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i) \\ \\ \\ \\ \\ - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i) \\\\ -P(Y_i=2|\\vec{x}_i).P(Y_i=1|\\vec{x}_i) \\ \\ \\ \\ \\ \\ P(Y_i=2|\\vec{x}_i).P(Y_i\\neq2|\\vec{x}_i) \\end{bmatrix}$.\n",
    "\n",
    "Now let,\n",
    "$$\\begin{aligned}\n",
    "P &= \\begin{bmatrix}\n",
    "P(Y_i=1|\\vec{x}_i).P(Y_i\\neq1|\\vec{x}_i) \\ \\ \\ \\ \\ - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i) \\\\ \n",
    "-P(Y_i=2|\\vec{x}_i).P(Y_i=1|\\vec{x}_i) \\ \\ \\ \\ \\ \\ P(Y_i=2|\\vec{x}_i).P(Y_i\\neq2|\\vec{x}_i) \n",
    "\\end{bmatrix} \\\\ \n",
    "\\\\ \n",
    "&= \\begin{bmatrix}\n",
    "P(Y_i=1|\\vec{x}_i).[1-P(Y_i=1|\\vec{x}_i)] & - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i) \\\\ \n",
    "-P(Y_i=2|\\vec{x}_i).P(Y_i=1|\\vec{x}_i) & P(Y_i=2|\\vec{x}_i).[1-P(Y_i=2|\\vec{x}_i)]\n",
    "\\end{bmatrix} \\\\ \n",
    "\\\\\n",
    "&= \\begin{bmatrix}\n",
    "P(Y_i=1|\\vec{x}_i)-P^2(Y_i=1|\\vec{x}_i) & - P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i) \\\\ \n",
    "-P(Y_i=2|\\vec{x}_i).P(Y_i=1|\\vec{x}_i) & P(Y_i=2|\\vec{x}_i)-P^2(Y_i=2|\\vec{x}_i)]\n",
    "\\end{bmatrix} \\\\ \n",
    "\\\\\n",
    "&= \\begin{bmatrix}\n",
    "P(Y_i=1|\\vec{x}_i) & 0 \\\\ \n",
    "0 & P(Y_i=2|\\vec{x}_i)\n",
    "\\end{bmatrix} - \n",
    "\\begin{bmatrix}\n",
    "P^2(Y_i=1|\\vec{x}_i) & P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i) \\\\ \n",
    "P(Y_i=1|\\vec{x}_i).P(Y_i=2|\\vec{x}_i) & P^2(Y_i=2|\\vec{x}_i)\\end{bmatrix} \\\\ \n",
    "\\\\ \n",
    "&= \\begin{bmatrix}\n",
    "P(Y_i=1|\\vec{x}_i) & 0 \\\\ \n",
    "0 & P(Y_i=2|\\vec{x}_i)\n",
    "\\end{bmatrix} - \n",
    "\\begin{bmatrix}\n",
    "P(Y_i=1|\\vec{x}_i) \\\\ \n",
    "P(Y_i=2|\\vec{x}_i)\\end{bmatrix}.\n",
    "\\begin{bmatrix}\n",
    "P(Y_i=1|\\vec{x}_i) \\\\ \n",
    "P(Y_i=2|\\vec{x}_i)\\end{bmatrix}^T\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now, to prove the positive semi-definiteness, we have to show for any $z\\neq0$, $\\vec{z}^T.P.\\vec{z}\\geq0$. So,\n",
    "$$\\begin{aligned}\n",
    "&  \\vec{z}^T.P.\\vec{z} \\\\ \n",
    "&= \\vec{z}^T.\\begin{bmatrix}\n",
    "P(Y_i=1|\\vec{x}_i) & 0 \\\\ \n",
    "0 & P(Y_i=2|\\vec{x}_i)\n",
    "\\end{bmatrix}.\\vec{z} - \\vec{z}^T.\n",
    "\\begin{bmatrix}\n",
    "P(Y_i=1|\\vec{x}_i) \\\\ \n",
    "P(Y_i=2|\\vec{x}_i)\\end{bmatrix}.\n",
    "\\begin{bmatrix}\n",
    "P(Y_i=1|\\vec{x}_i) \\\\ \n",
    "P(Y_i=2|\\vec{x}_i)\\end{bmatrix}^T.\\vec{z} \\\\ \n",
    "\\\\\n",
    "&= \\sum_{j=1}^2P(Y_i=j|\\vec{x}_i)z_j^2 \\ \\ - \\ \\ \\left(\\sum_{j=1}^2P(Y_i=j|\\vec{x}_i)z_j\\right)^2  \\end{aligned}$$\n",
    "\n",
    "Now, by **Cauchy-Schwarz Inequality**, $$\\left(\\sum_{j=1}^l u_jv_j\\right)^2 \\ \\ \\leq \\ \\ \\sum_{j=1}^l u_j^2.\\sum_{j=1}^lv_j^2$$\n",
    "\n",
    "If we set, l =2, $u_j=\\sqrt{P(Y_i=j|\\vec{x}_i)}$ also as, $P(Y_i=j|\\vec{x}_i) \\geq 0$, we can set $v_j=\\sqrt{P(Y_i=j|\\vec{x}_i)}.z_j$; we get,\n",
    "$$\\begin{aligned}\n",
    " &\\left(\\sum_{j=1}^2 \\sqrt{P(Y_i=j|\\vec{x}_i)}.\\sqrt{P(Y_i=j|\\vec{x}_i)}.z_j\\right)^2 \\leq \\sum_{j=1}^2P(Y_i=j|\\vec{x}_i).\\sum_{j=1}^2P(Y_i=j|\\vec{x}_i).z_j^2 \\\\ \n",
    " \\implies & \\left(\\sum_{j=1}^2 P(Y_i=j|\\vec{x}_i).z_j\\right)^2 \\leq \\sum_{j=1}^2P(Y_i=j|\\vec{x}_i).z_j^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "As, $\\sum_{j=1}^2P(Y_i=j|\\vec{x}_i) = 1$.\n",
    "\n",
    "So, $\\vec{z}^T.P.\\vec{z} \\geq 0$. So, P is positive semi-definite.\n",
    "\n",
    "Using the above steps we can prove that -H is negative semi-definite, so H is positive semi-definite. Which in term proves that, if we solve the equation $\\frac{\\partial}{\\partial \\vec{\\beta}}log L = 0$ we will always get one solution and the solution will be point of maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035be6c5",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this article we have seen mainly theory behind Multionomial Logistic Regression including, choosing right distribution for the problem, build the set equations to get the parameter estimates using MLE also  proved concavity of the problem. Here we are not actually solving the set of equations of MLE, the reason is, we cannot get closed form solution from the equations, we have to use numerical methods. As we have not discussed any of it we are leaving it for future articles."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Multinomial_Logistic_Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
