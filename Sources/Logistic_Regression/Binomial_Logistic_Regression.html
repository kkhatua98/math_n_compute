<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>Binomial Logistic Regression</title>

    <!-- Linking css and javascript -->
    <link rel="stylesheet" id="cssfile" href="../../styles/lightmode.css">
    <script type="text/javascript" src="../../scripts/functionality.js"></script>

    <!-- Linking for MathJax to load -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

</head>

<body>
    <!-- <button id="dmbutton" class="fixedContainer">Dark Mode</button> -->

    
    <!-- <div style="background: #222; height: 70px; text-align: center; display:flex; justify-content: space-around;" class="header">
        <button style="background: #222; height:80%; outline:none">
            <svg width="25" height="25" fill="white" class="bi bi-list" viewBox="0 0 16 16" style="height:100%; vertical-align:middle">
                <path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5"/>
            </svg>
        </button>
        <a href="../../index.html" style=" text-align: center; vertical-align: middle; text-decoration: none; color:rgb(158, 153, 153)">
            <svg width="25" height="25" fill="currentColor" class="bi bi-house" viewBox="0 0 16 16" weight="bold" style="height:100%; vertical-align:middle">
                <path d="M8.707 1.5a1 1 0 0 0-1.414 0L.646 8.146a.5.5 0 0 0 .708.708L2 8.207V13.5A1.5 1.5 0 0 0 3.5 15h9a1.5 1.5 0 0 0 1.5-1.5V8.207l.646.647a.5.5 0 0 0 .708-.708L13 5.793V2.5a.5.5 0 0 0-.5-.5h-1a.5.5 0 0 0-.5.5v1.293zM13 7.207V13.5a.5.5 0 0 1-.5.5h-9a.5.5 0 0 1-.5-.5V7.207l5-5z"/>
            </svg>
            <span style="height:100%; vertical-align:middle; font-weight:bold;">Home</span>
        </a>
        <button style="background: #222; height:80%; display:none">
            <svg width="25" height="25" fill="white" class="bi bi-list" viewBox="0 0 16 16" style="height:100%; vertical-align:middle">
                <path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5"/>
            </svg>
        </button>
    </div> -->
    
    <div class="container">
        <!-- <ul class="toc">
            <li class="Heading"><a href="#Section1">Introduction</a></li>
            <li class="SubHeading"><a href="#Section11">Case 1</a></li>
            <li class="SubHeading"><a href="#Section12">Case 2</a></li>
            <li class="Heading"><a href="#Section2">Doing the Mathematics</a></li>
            <li class="SubHeading"><a href="#Section21">First Axis</a></li>
            <li class="SubHeading"><a href="#Section22">Second Axis</a></li>
        </ul> -->
        

        
        <ul class="toc" id="toc">
            <a href="../../index.html" style="margin-bottom: 25px; margin-left:5px; text-align: center; vertical-align: middle;">
                <svg width="25" height="25" fill="currentColor" class="bi bi-house" viewBox="0 0 16 16" weight="bold" style="height:100%; vertical-align:middle">
                    <path d="M8.707 1.5a1 1 0 0 0-1.414 0L.646 8.146a.5.5 0 0 0 .708.708L2 8.207V13.5A1.5 1.5 0 0 0 3.5 15h9a1.5 1.5 0 0 0 1.5-1.5V8.207l.646.647a.5.5 0 0 0 .708-.708L13 5.793V2.5a.5.5 0 0 0-.5-.5h-1a.5.5 0 0 0-.5.5v1.293zM13 7.207V13.5a.5.5 0 0 1-.5.5h-9a.5.5 0 0 1-.5-.5V7.207l5-5z"/>
                </svg>
                <span style="height:100%; vertical-align:middle">Home</span>
            </a>
            <!-- <details style="background: #222">
                <summary style="display: flex; justify-content: center; padding: 5px;">
                    <a href="../Linear_Regression/Multicollinearity/Multicollinearity.html" style="flex: 1 0 100%; text-align: center;">
                        <svg width="25" height="25" fill="currentColor" class="bi bi-house" viewBox="0 0 16 16" weight="bold">
                            <path d="M8.707 1.5a1 1 0 0 0-1.414 0L.646 8.146a.5.5 0 0 0 .708.708L2 8.207V13.5A1.5 1.5 0 0 0 3.5 15h9a1.5 1.5 0 0 0 1.5-1.5V8.207l.646.647a.5.5 0 0 0 .708-.708L13 5.793V2.5a.5.5 0 0 0-.5-.5h-1a.5.5 0 0 0-.5.5v1.293zM13 7.207V13.5a.5.5 0 0 1-.5.5h-9a.5.5 0 0 1-.5-.5V7.207l5-5z"/>
                        </svg>
                    </a>
                </summary>
            </details> -->
            <details>
                <summary>Linear Regression</summary>
                <details style="margin-top:8px;margin-left:2px;margin-right:2px">
                    <summary
                        style="padding-left:40px; padding-top:5px; padding-bottom:8px; color:rgb(194, 191, 191); border-radius:5px">
                        Multicollinearity
                    </summary>
                    <ul>
                        <!-- <li class="Heading"><a href="#Section1" style="margin-top:0px;">Binomial Logistic Regression</a></li> -->
                        <li class="SubHeading"><a href="../Linear_Regression/Multicollinearity/Multicollinearity.html#Section1">Variance of Coefficient Estimates</a></li>
                        <li class="SubHeading"><a href="../Linear_Regression/Multicollinearity/Multicollinearity.html#Section2">Deducing Diagonal Elements
                        </a></li>
                    </ul>
                </details>
            </details>
            <details open>
                <summary>Logistic Regression</summary>
                <details style="margin-top:8px;margin-left:2px;margin-right:2px" open>
                    <summary
                        style="padding-left:40px; padding-top:5px; padding-bottom:8px; color:rgb(194, 191, 191); border-radius:5px">
                        Binomial Logistic Regression
                    </summary>
                    <ul>
                        <!-- <li class="Heading"><a href="#Section1" style="margin-top:0px;">Binomial Logistic Regression</a></li> -->
                        <li class="SubHeading"><a href="Binomial_Logistic_Regression.html#Section1">Mathematical
                                Representation</a></li>
                        <li class="SubHeading"><a href="Binomial_Logistic_Regression.html#Section2">Coefficient
                                Estimates by MLE</a></li>
                        <li class="SubSubHeading"><a
                                href="Binomial_Logistic_Regression.html#Section21">Log-Likelihood</a></li>
                        <li class="SubSubHeading"><a href="Binomial_Logistic_Regression.html#Section22">First
                                Derivative</a></li>
                        <li class="SubSubHeading"><a href="Binomial_Logistic_Regression.html#Section23">Hessian
                                Matrix</a></li>
                    </ul>
                </details>
                <details style="margin-top:8px; margin-left:2px;margin-right:2px">
                    <summary
                        style="padding-left:40px; padding-top:5px; padding-bottom:8px; color:rgb(194, 191, 191); border-radius:5px">
                        Multinomial Logistic Regression
                    </summary>
                    <ul>
                        <!-- <li class="Heading"><a href="#Section1" style="margin-top:0px;">Binomial Logistic Regression</a></li> -->
                        <li class="SubHeading"><a href="Multinomial_Logistic_Regression.html#Section1">Mathematical
                                Representation</a></li>
                        <li class="SubHeading"><a href="Multinomial_Logistic_Regression.html#Section2">Coefficient
                                Estimates by MLE</a></li>
                        <li class="SubSubHeading"><a
                                href="Multinomial_Logistic_Regression.html#Section21">Log-Likelihood</a></li>
                        <li class="SubSubHeading"><a href="Multinomial_Logistic_Regression.html#Section22">First
                                Derivative</a></li>
                        <li class="SubSubHeading"><a href="Multinomial_Logistic_Regression.html#Section23">Solve for the
                                Coefficients</a></li>
                        <li class="SubHeading"><a href="Multinomial_Logistic_Regression.html#Section3">Proving Concavity
                                of the Problem</a></li>
                        <li class="SubSubHeading"><a href="Multinomial_Logistic_Regression.html#Section31">Deriving
                                Hessian Matrix</a></li>
                        <li class="SubSubHeading"><a href="Multinomial_Logistic_Regression.html#Section32">Concavity of
                                Hessian Matrix</a></li>
                        <li class="SubHeading"><a href="Multinomial_Logistic_Regression.html#Section4">Conclusion</a>
                        </li>
                    </ul>
                </details>
            </details>
            <details>
                <summary>Principal Component Analysis</summary>
                <ul>
                    <li class="Heading"><a href="../PCA/PCA.html#Section1" style="margin-top:0px;">Introduction</a></li>
                    <li class="SubHeading"><a href="../PCA/PCA.html#Section11">Case 1</a></li>
                    <li class="SubHeading"><a href="../PCA/PCA.html#Section12">Case 2</a></li>
                    <li class="Heading"><a href="../PCA/PCA.html#Section2">Doing the Mathematics</a></li>
                    <li class="SubHeading"><a href="../PCA/PCA.html#Section21">First Axis</a></li>
                    <li class="SubHeading"><a href="../PCA/PCA.html#Section22">Second Axis</a></li>
                </ul>
            </details>
        </ul>

        <!-- <div class="toc_sidebar">
            >
        </div> -->



        
        <div class="content">
            <ul class="mini_toc" id="mini_toc">
                <li>
                    <button style="background: #222; height:100%; outline:none" id="mini_toc_button">
                        <svg width="25" height="25" fill="white" class="bi bi-list" viewBox="0 0 16 16" style="height:100%; vertical-align:middle">
                            <path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5"/>
                        </svg>
                    </button>
                </li>
                <li>
                    <a href="../../index.html" style=" text-align: center; vertical-align: middle; text-decoration: none; color:rgb(158, 153, 153)">
                        <svg width="25" height="25" fill="currentColor" class="bi bi-house" viewBox="0 0 16 16" weight="bold" style="height:100%; vertical-align:middle">
                            <path d="M8.707 1.5a1 1 0 0 0-1.414 0L.646 8.146a.5.5 0 0 0 .708.708L2 8.207V13.5A1.5 1.5 0 0 0 3.5 15h9a1.5 1.5 0 0 0 1.5-1.5V8.207l.646.647a.5.5 0 0 0 .708-.708L13 5.793V2.5a.5.5 0 0 0-.5-.5h-1a.5.5 0 0 0-.5.5v1.293zM13 7.207V13.5a.5.5 0 0 1-.5.5h-9a.5.5 0 0 1-.5-.5V7.207l5-5z"/>
                        </svg>
                        <span style="height:100%; vertical-align:middle; font-weight:bold;">Home</span>
                    </a>
                </li>
                <li>
                    <button style="background: #222; height:95%; outline:none">
                        <svg width="25" height="25" fill="white" class="bi bi-list" viewBox="0 0 16 16" style="height:100%; vertical-align:middle">
                            <path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5"/>
                        </svg>
                    </button>
                </li>
            </ul>



            <div class="actual_content">
                <h1 id="title">Binomial Logistic Regression</h1>
    
                <p>Let \(\vec{x} = (x_1, x_2, x_3, ..., x_p)^T\) is the p dimensional independent vector and \(y\) is the
                    categorical dependent variable. For now suppose \(y\) has only two categories (for more than two
                    categories mathematics is little different). We will always represent categories by 0 and 1. In some
                    cases to do that we may have to use appropriate technic to represent categories by 0 and 1. For example,
                    if categories are boy or girl then we can represent 0 for boy and 1 for girl, alternatively 0 for girl
                    and 1 for boy.</p>
    
                <h2 id="Section1"> Mathematical Representation</h2>
                <p>Now,we may try to fit a linear regression model of the form,
                    \(y_i=\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + ... + \beta_p x_{ip} + \epsilon_i\)
                    then, as all \(y\)s are not same, mix of 0 and 1, all the coefficient estimates can not be zero at the
                    same time; ie. the regression plane would have non-zero slope along some axis. So, for suitable choice
                    of \(x\) vector predicted value of y would be greater than 0, similarly for suitable choice of x
                    predicted value of y can be negative (becomes unbounded briefly said). So, what we do is, we set,</p>
                <div class="wrap">
                    \[y_i \sim Bernoulli(p_i)\],</div>
                <p>where,</p>
                <div class="wrap"> \[p_i = P(y_i = 1) = \frac{1}{1+ exp[-(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} +
                    ... + \beta_p x_{ip})]}\]</div>
                <p>and estimate the parameters by **MLE**.</p>
    
                <h2 id="Section2"> Coefficient Estimates by MLE</h2>
                <h3 id="Section21"> Form of the Log Likelihood</h3>
                <p>Likelihood,</p>
                <div class="wrap">
                    \[\begin{aligned}
                    L &= \prod_{i=1}^nP(y_i) \\
                    &= \prod_{i=1}^np_i^{y_i}(1 - p_i)^{1 - y_i}
                    \end{aligned}\]
                </div>
    
                <p>So, log-likelihood,</p>
                <div class="wrap">\[\begin{aligned}
                    l &= \log L \\
                    &= \log \prod_{i=1}^np_i^{y_i}(1 - p_i)^{1 - y_i} \\
                    &= \sum_{i=1}^n y_i\log p_i + (1-y_i)\log(1-p_i) \\
                    &= \sum_{i=1}^n y_i.\log \frac{1}{1+ exp[-\vec{\beta}^T.\vec{x}_i]} + (1-y_i).\log
                    \frac{exp[-\vec{\beta}^T.\vec{x}_i]}{1+ exp[-\vec{\beta}^T.\vec{x}_i]} \\
                    &= \sum_{i=1}^n -y_i.\log (1+ exp[-\vec{\beta}^T.\vec{x}_i]) + (1-y_i).\log
                    exp[-\vec{\beta}^T.\vec{x}_i] \ - (1-y_i).\log (1+ exp[-\vec{\beta}^T.\vec{x}_i])\\
                    &= \sum_{i=1}^n -y_i.\log (1+ exp[-\vec{\beta}^T.\vec{x}_i]) - (1-y_i).\vec{\beta}^T.\vec{x}_i \ -
                    (1-y_i).\log (1+ exp[-\vec{\beta}^T.\vec{x}_i])\\
                    &= \sum_{i=1}^n - (1-y_i).\vec{\beta}^T.\vec{x}_i \ - \log (1+ exp[-\vec{\beta}^T.\vec{x}_i])\\
                    &= -\sum_{i=1}^n (1-y_i).\vec{\beta}^T.\vec{x}_i \ + \log (1+ exp[-\vec{\beta}^T.\vec{x}_i])
                    \end{aligned}
                    \]
                </div>
    
                <p>We will get the solution by solving \(\frac{\partial l}{\partial \vec{\beta}} = 0\); </p>
    
                <h3 id="Section22"> Deriving the First Derivative</h3>
                <p>Now,</p>
                <div class="wrap">
                    \[\begin{aligned}
                    \frac{\partial l}{\partial \vec{\beta}} &= \frac{\partial}{\partial \vec{\beta}} -\sum_{i=1}^n
                    (1-y_i).\vec{\beta}^T.\vec{x}_i \ + \log (1+ exp[-\vec{\beta}^T.\vec{x}_i]) \\
                    &= -\sum_{i=1}^n (1-y_i).\frac{\partial}{\partial \vec{\beta}} \vec{\beta}^T.\vec{x}_i \ +
                    \frac{\partial}{\partial \vec{\beta}}\log (1+ exp[-\vec{\beta}^T.\vec{x}_i]) \\
                    &= -\sum_{i=1}^n (1-y_i).\vec{x}_i \ + \frac{1}{1+
                    exp[-\vec{\beta}^T.\vec{x}_i]}.exp[-\vec{\beta}^T.\vec{x}_i].(-1).\vec{x}_i \\
                    &= -\sum_{i=1}^n (1-y_i).\vec{x}_i \ + \sum_{i=1}^n \frac{exp[-\vec{\beta}^T.\vec{x}_i]}{1+
                    exp[-\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i \\
                    &= -\sum_{i=1}^n (1-y_i).\vec{x}_i \ + \sum_{i=1}^n \frac{1}{1+ exp[\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i
                    \end{aligned}
                    \]
                </div>
    
                <p>
                    Now, \(\frac{\partial l}{\partial \beta} = 0\) will produce global maximum, if we prove that \(l\) is
                    concave function with respect to the model coefficients, ie. the **Hessian Matrix** is negative
                    definite.</p>
                <div class="wrap">
                    \[H = \begin{pmatrix}\begin{pmatrix}\frac{\partial^2 l}{\partial \beta_n
                    \beta_m}\end{pmatrix}\end{pmatrix} = \frac{\partial }{\partial \vec{\beta}}\left[\frac{\partial
                    l}{\partial \vec{\beta}}\right]^T < 0\]
                </div>


                <h3 id="Section23"> Deriving the Hessian Matrix</h3>
                <p>Here,</p>
                <div class="wrap">
                    \[\begin{aligned}
                    H &= \frac{\partial }{\partial \vec{\beta}}\left[\frac{\partial l}{\partial
                    \vec{\beta}}\right]^T \\
                    &= \frac{\partial }{\partial \vec{\beta}} \left[-\sum_{i=1}^n (1-y_i).\vec{x}_i \ + \sum_{i=1}^n
                    \frac{1}{1+ exp[\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i \right]^T \\
                    &= -\frac{\partial }{\partial \vec{\beta}} \sum_{i=1}^n (1-y_i).\vec{x}_i^T \ + \frac{\partial
                    }{\partial \vec{\beta}}\sum_{i=1}^n \frac{1}{1+ exp[\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i^T \\
                    &= 0 \ \ + \sum_{i=1}^n \left[\frac{\partial }{\partial \vec{\beta}} \frac{1}{1+
                    exp[\vec{\beta}^T.\vec{x}_i]}\right].\vec{x}_i^T \\
                    &= \sum_{i=1}^n \left[ \frac{-1}{\left(1+
                    exp[\vec{\beta}^T.\vec{x}_i]\right)^2}.exp[\vec{\beta}^T.\vec{x}_i].\vec{x}_i\right]\vec{x}_i^T
                    \\
                    &= -\sum_{i=1}^n \frac{1}{1+
                    exp[\vec{\beta}^T.\vec{x}_i]}.\frac{exp[\vec{\beta}^T.\vec{x}_i]}{1+
                    exp[\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i.\vec{x}_i^T \\
                    &= -\sum_{i=1}^n \frac{exp[-\vec{\beta}^T.\vec{x}_i]}{1+
                    exp[-\vec{\beta}^T.\vec{x}_i]}.\frac{1}{1+ exp[-\vec{\beta}^T.\vec{x}_i]}.\vec{x}_i.\vec{x}_i^T
                    \\
                    &= -\sum_{i=1}^n p_i.(1-p_i).\vec{x}_i.\vec{x}_i^T \\
                    &= -\sum_{i=1}^n \sqrt{p_i.(1-p_i)}.\vec{x}_i.\sqrt{p_i.(1-p_i)}.\vec{x}_i^T \\
                    &= -\sum_{i=1}^n \vec{x^*}_i .\vec{x^*}_i^T \ \ \leq \ \ 0
                    \end{aligned}\]
                </div>


                <p>
                    Here, \(\vec{x^*}_i = \sqrt{p_i.(1-p_i)}.\vec{x}_i\). The last equality holds for the fact that \(\vec{x^*}_i .\vec{x^*}_i^T\) is positive definite and sum of positive definite metrices is
                            positive definite.
                </p>
                <p>
                    Now if we set \(\frac{\partial l}{\partial \beta_m} = 0\), we cannot get explicit form of
                    \(\beta\) due the complicated form of \(\frac{\partial l}{\partial \beta_m}\), so to get the
                    solution we have to use numerical methods.
                </p>
    
                <div class="navigator">
                    <span><a href="../Linear_Regression/Multicollinearity/Multicollinearity.html">&#8249;</a><span style="margin-left:5px">Multicollinearity</span></span>
                    <span style="margin-right:5px"><span>Multinomial Logistic Regression</span><a href="Multinomial_Logistic_Regression.html">&#8250;</a></span>
                </div>
            </div>
            <!-- </div> -->
        </div>


</body>

</html>