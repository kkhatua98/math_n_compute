{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-XNZgt1L2he-"
      },
      "source": [
        "# Why Multicollinearity Increases Variance of Coefficient Estimates\n",
        "## Variance of Coefficient Estimates\n",
        "We know that for the model, $Y=X\\vec{\\beta}+\\vec{\\epsilon}$ coefficient estimates $\\hat{\\vec{\\beta}}$ has variance,\n",
        "$$var(\\hat{\\vec{\\beta}})=\\sigma^2(X^TX)^{-1}$$\n",
        "Now, let us assume that multicollinearity is present in the variables $x_1,...,x_p$ and all of them are involved in it. So, in simple terms it means that, if we fit linear regression of any of the $x_i$s on rest of the $x$s then the quality of the fit will be very good. Now, We have to prove that this good fit somehow increases the variance of the coefficient estimates, or it increases the values of the diagonal elements of $var(\\hat{\\vec{\\beta}})$.\n",
        "\n",
        "We will go about this problem in this way, we will try to deduce the diagonal elements of $var(\\hat{\\vec{\\beta}})$ in such a form that, where there will be a term which increases due the good regression fit between $x$ variables.\n",
        "## Deducing Diagonal Elements of $var(\\hat{\\vec{\\beta}})$\n",
        "We will do the calculation for $x_1$ and show that the first diagonal element of $var(\\hat{\\beta})$ is getting increased because of the good regression fit between $x_1$ and other $x$ variables. To show that variance is increasing for  rest of the variables what we can do is that, we will swap $x_1$ with other $x$s one by one.\n",
        "$$\\begin{aligned}\n",
        "var(\\hat{\\vec{\\beta}}) &= \\sigma^2(X^TX)^{-1} \\\\\n",
        "\\\\ \n",
        "&=\\sigma^2\\begin{bmatrix}\\begin{pmatrix}\\vec{x}_1^T \\\\ \n",
        "\\vec{x}_2^T \\\\ \n",
        ". \\\\ \n",
        ". \\\\ \n",
        ". \\\\ \n",
        "\\vec{x}_p^T\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "\\vec{x}_1 \\ \\ \\ \\ \\vec{x}_2 \\ \\ \\ \\ ... \\ \\ \\ \\ \\vec{x}_p\n",
        "\\end{pmatrix}\n",
        "\\end{bmatrix}^{-1} \\\\ \n",
        "\\\\ \n",
        "&=\\sigma^2\\begin{pmatrix}\\vec{x}_1^T\\vec{x}_1 \\ \\ \\ \\ \\vec{x}_1^T\\vec{x}_2 \\ \\ \\ \\ ... \\ \\ \\ \\ \\vec{x}_1^T\\vec{x}_p \\\\ \n",
        "\\vec{x}_2^T\\vec{x}_1 \\ \\ \\ \\ \\vec{x}_2^T\\vec{x}_2 \\ \\ \\ \\ ... \\ \\ \\ \\ \\vec{x}_2^T\\vec{x}_p \\\\ \n",
        ". \\\\ \n",
        ". \\\\ \n",
        ". \\\\\n",
        "\\vec{x}_p^T\\vec{x}_1 \\ \\ \\ \\ \\vec{x}_p^T\\vec{x}_2 \\ \\ \\ \\ ... \\ \\ \\ \\ \\vec{x}_p^T\\vec{x}_p \\\\ \n",
        "\\end{pmatrix}^{-1} \\\\ \n",
        "\\\\ \n",
        "&=\\sigma^2\\begin{pmatrix}\\vec{x}_1^T\\vec{x}_1 & \\vec{x}_1^TX_{(1)} \\\\ \n",
        "X_{(1)}^T\\vec{x}_1 & X_{(1)}^TX_{(1)}\\end{pmatrix}^{-1}\n",
        "\\end{aligned}$$\n",
        "If we calculate the above inverse matrix and get the frist diagonal element, we will get the variance of $x_1$.\n",
        "\n",
        "Now for any block matrix,\n",
        "$$ P=\\begin{bmatrix} A &  B \\\\  C &  D\\end{bmatrix}^{-1}=\\begin{bmatrix}\\left( A- B D^{-1}  C\\right)^{-1} & -\\left( A- B D^{-1}  C\\right)^{-1}  B D^{-1} \\\\ - D^{-1}  C\\left( A- B D^{-1}  C\\right)^{-1} &  D^{-1}+ D^{-1}  C\\left( A- B D^{-1}  C\\right)^{-1}  B D^{-1}\\end{bmatrix}$$\n",
        "If we put, $A=\\vec{x}_1^T\\vec{x}_1$, $B=\\vec{x}_1^TX_{(1)}$, $C=X_{(1)}^T\\vec{x}_1$ and $D=X_{(1)}^TX_{(1)}$, we will get $var(\\hat{\\vec{\\beta}})$. Now in this expression our interest is only on $\\left( A- B D^{-1}  C\\right)^{-1}$. If we replace $A, B, C, D$ with their appropriate values we will get,\n",
        "$$\\begin{aligned}\n",
        "&\\left( A- B D^{-1}  C\\right)^{-1} \\\\ \n",
        "=& \\ \\left[\\vec{x}_1^T\\vec{x}_1-\\vec{x}_1^TX_{(1)}\\left[X_{(1)}^TX_{(1)}\\right]^{-1}X_{(1)}^T\\vec{x}_1\\right]^{-1} \\\\ \n",
        "= & \\ \\left[\\vec{x}_1^T\\vec{x}_1-\\vec{x}_1^TX_{(1)}\\left[X_{(1)}^TX_{(1)}\\right]^{-1}X_{(1)}^T\\vec{x}_1 - \\underbrace{ \\left[\\left(X_{(1)}^TX_{(1)}\\right)^{-1}X_{(1)}^T\\vec{x}_1\\right]^TX_{(1)}^T\\vec{x}_1}_{subtracting} + \\underbrace{\\left[\\left(X_{(1)}^TX_{(1)}\\right)^{-1}X_{(1)}^T\\vec{x}_1\\right]^TX_{(1)}^T\\vec{x}_1}_{adding}\\right]^{-1} \\\\ \n",
        "= & \\ \\left[\\vec{x}_1^T\\vec{x}_1-\\vec{x}_1^TX_{(1)}\\left[X_{(1)}^TX_{(1)}\\right]^{-1}X_{(1)}^T\\vec{x}_1 - \\left[\\left(X_{(1)}^TX_{(1)}\\right)^{-1}X_{(1)}^T\\vec{x}_1\\right]^TX_{(1)}^T\\vec{x}_1 + \\left[\\left(X_{(1)}^TX_{(1)}\\right)^{-1}X_{(1)}^T\\vec{x}_1\\right]^T\\left(X_{(1)}^TX_{(1)}\\right)\\left(X_{(1)}^TX_{(1)}\\right)^{-1}X_{(1)}^T\\vec{x}_1\\right]^{-1} \\\\ \n",
        "= & \\ \\left[\\vec{x}_1^T\\vec{x}_1-\\vec{x}_1^TX_{(1)}\\left[X_{(1)}^TX_{(1)}\\right]^{-1}X_{(1)}^T\\vec{x}_1 - \\vec{x}_1^TX_{(1)}\\left(X_{(1)}^TX_{(1)}\\right)^{-1}X_{(1)}^T\\vec{x}_1 + \\left[\\left(X_{(1)}^TX_{(1)}\\right)^{-1}X_{(1)}^T\\vec{x}_1\\right]^T\\left(X_{(1)}^TX_{(1)}\\right)\\left(X_{(1)}^TX_{(1)}\\right)^{-1}X_{(1)}^T\\vec{x}_1\\right]^{-1} \\\\ \n",
        "= \\ & \\left[\\vec{x}_1^T\\vec{x}_1 - \\vec{x}_1^TX_{(1)}\\hat{\\vec{\\beta}}_{(1)}- \\vec{x}_1^TX_{(1)}\\hat{\\vec{\\beta}}_{(1)}+\\hat{\\vec{\\beta}}_{(1)}^T\\left(X_{(1)}^TX_{(1)}\\right)\\hat{\\vec{\\beta}}_{(1)}\\right]^{-1} \\\\ \n",
        "= \\ & \\left[\\vec{x}_1^T\\left(\\vec{x}_1-X_{(1)}\\hat{\\vec{\\beta}}_{(1)}\\right) - \\underbrace{\\left(\\vec{x}_1^T-\\hat{\\vec{\\beta}}_{(1)}^TX_{(1)}^T\\right)}_{a}\\underbrace{X_{(1)}\\hat{\\vec{\\beta}}_{(1)}}_{b}\\right]^{-1} \\\\ \n",
        "= \\ & \\left[\\vec{x}_1^T\\left(\\vec{x}_1-X_{(1)}\\hat{\\vec{\\beta}}_{(1)}\\right) - \\underbrace{\\hat{\\vec{\\beta}}_{(1)}^TX_{(1)}^T\\left(\\vec{x}_1-X_{(1)}\\hat{\\vec{\\beta}}_{(1)}\\right)}_{\\text{swap a, b as both are vectors}}\\right]^{-1} \\\\ \n",
        "= \\ & \\left[\\left(\\vec{x}_1^T-\\hat{\\vec{\\beta}}_{(1)}^TX_{(1)}^T\\right)\\left(\\vec{x}_1-X_{(1)}\\hat{\\vec{\\beta}}_{(1)}\\right)\\right]^{-1} \\\\ \n",
        "= & \\ \\left[\\left(\\vec{x}_1-X_{(1)}\\hat{\\vec{\\beta}}_{(1)}\\right)^T\\left(\\vec{x}_1-X_{(1)}\\hat{\\vec{\\beta}}_{(1)}\\right)\\right]^{-1} \\\\ \n",
        "= & \\ \\frac{1}{\\text{SSE of regression of $x_1$ on rest of the xs}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Now as the quality of regression between $x_1$ and other $x$ s are good so the SSE would be small, and for this reason the above term or the first diagonal matrix of the variance matrix or variance of $x_1$ increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQmeqQ132ed8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
